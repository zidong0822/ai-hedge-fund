services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Apple Silicon GPU acceleration
      - METAL_DEVICE=on
      - METAL_DEVICE_INDEX=0
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

  hedge-fund:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-hedge-fund
    depends_on:
      - ollama
    volumes:
      - ../.env:/app/.env
    command: python src/main.py --ticker AAPL,MSFT,NVDA
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
    tty: true
    stdin_open: true

  hedge-fund-reasoning:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-hedge-fund
    depends_on:
      - ollama
    volumes:
      - ../.env:/app/.env
    command: python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
    tty: true
    stdin_open: true

  hedge-fund-ollama:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-hedge-fund
    depends_on:
      - ollama
    volumes:
      - ../.env:/app/.env
    command: python src/main.py --ticker AAPL,MSFT,NVDA --ollama
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
    tty: true
    stdin_open: true

  backtester:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-hedge-fund
    depends_on:
      - ollama
    volumes:
      - ../.env:/app/.env
    command: python src/backtester.py --ticker AAPL,MSFT,NVDA
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
    tty: true
    stdin_open: true

  backtester-ollama:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-hedge-fund
    depends_on:
      - ollama
    volumes:
      - ../.env:/app/.env
    command: python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONPATH=/app
    tty: true
    stdin_open: true

  # 添加前端服务（使用Node镜像）
  frontend:
    image: node:18-alpine
    working_dir: /app
    ports:
      - "5173:5173"
    volumes:
      - ../app/frontend:/app
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0"
    restart: unless-stopped

  # 添加后端API服务
  backend-api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ../.env:/app/.env
    command: poetry run uvicorn app.backend.main:app --host 0.0.0.0 --port 8000 --reload
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
    restart: unless-stopped

volumes:
  ollama_data:
